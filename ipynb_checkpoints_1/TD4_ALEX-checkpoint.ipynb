{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Fashion-MNIST\n",
    "\n",
    "Exercise: Use the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a drop-in replacement for the MNIST dataset. \n",
    "MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. \n",
    "\n",
    "Fashion-MNIST is a set of 28x28 greyscale images of clothes. It's more complex than MNIST, so it's a better representation of the actual performance of your network,\n",
    "and a better representation of datasets you'll use in the real world.\n",
    "\n",
    "<img src='TD_images/fashion-mnist-sprite.png' width=500px>\n",
    "\n",
    "\n",
    "\n",
    "## load the dataset through torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import helper1\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAJWklEQVR4nO3dTW+c5RnF8Xvex/bEduKXCIdEaQmVSKAsSDcUiYqk/QLQbqD9aJVY0qwqVi2FDQkoQQLRVmXnWKSxQ4rr+m3Gz7x20U0r+T5XO4+mOUT/35KjZ+JMcvJIXLruuzKZTBIAP9Un/QMAOB3lBExRTsAU5QRMUU7AVF2Fb7z2Mv8rF5ixj25/WTntv/PmBExRTsAU5QRMUU7AFOUETFFOwBTlBExRTsAU5QRMUU7AFOUETFFOwBTlBExRTsAU5QRMUU7AFOUETFFOwBTlBExRTsAU5QRMUU7AFOUETFFOwBTlBExRTsAU5QRMUU7AFOUETFFOwJS8AhDfPT94/nmZjyf6Vsfj4+NsVqvW5LOHh4cy3z/Ylzn+E29OwBTlBExRTsAU5QRMUU7AFOUETFFOwBRzzqfMT15/XeZFUch8MBhks3Nnz8ln5+baMn/v1i2Zf/3gQTa7eeOGfPbFq9dkflKcyHx39+8yHw7z38t4PJbP/vb992Wew5sTMEU5AVOUEzBFOQFTlBMwRTkBU5QTMMWc8ynTL/oy73a7U3/2N4+/kfna6prMNzY2ZL60tJTNrr1wVT77cHtb5pOJnkVWg13V5eWz2ezoSO+xTos3J2CKcgKmKCdginICpignYIpyAqYYpTxl5hfmZd7t9WQ+EUdnRuOIg8MDma+v6VFL5/L3stn2zo58djgcyjxa6xqP9SrdeDzKZtHPNi3enIApygmYopyAKcoJmKKcgCnKCZiinIAp5pxPmUqlInM1r0sppWpV/XutP7vd1kdjHh/rdbWRmEXW63qlKyV9tWG12pR5v69X7VZWVrLZx3fuyGenxZsTMEU5AVOUEzBFOQFTlBMwRTkBU5QTMMWc8wlQs0i1T5lSSpcuXpR5q9WSebT3WK/P7q9ENINttfKzyF5PX+E3Gul9zcgkmJN2Op1spmfD0+PNCZiinIApygmYopyAKcoJmKKcgCnKCZhizvkERLNM5cevvirzx4//JvNo1lip5P+9bjT0X5fhQM9Q9/b2ZK5mrLVa9B7R32n0ndeCKwBHo/webGNGs2HenIApygmYopyAKcoJmKKcgCnKCZiinIAp5pxmLly4IPONjQ2Zb27el3l0tuxwmJ/nRXdcRtbX12Veq+VnjUWh9zmjncpmU59be3KiP39/P3/36NUXrspnv37wQOY5vDkBU5QTMEU5AVOUEzBFOQFTlBMwxSjFzE9v3JT5o0ePZB5dlRevq+Xz6FjNQU3nETUuUWOW/0a4MlbTVej1etns0iV9XOm0eHMCpignYIpyAqYoJ2CKcgKmKCdginICpphzTiFaT4pWq9SK0bmzZ+Wz97e2ZB4dXzkeTz/vGw4H8tnJpNxKmZplRnPOwSD62fTvO/ozHQz62azZ0OtoG888I/PszzTVUwBmjnICpignYIpyAqYoJ2CKcgKmKCdgijnnFKI5prrKLqWUfnbzRjbb3tku9dnqqrqU4isA1VV70a8dzxL1rz0a5fPoO68GV/hFv+/o84uiyGbH3a589sqVKzLP4c0JmKKcgCnKCZiinIApygmYopyAKcoJmGLOOQO/eucdmR8dH2ez6GzYaNYYjPPCvUc1D2zUG/LZUTiLDH44cWbuaKQ/u1LR75lo1zSac9bE914E1wcunlmUeQ5vTsAU5QRMUU7AFOUETFFOwBTlBExRTsDUTOec0Q6dEu0GRvO+aF5Yxi/fflvm8/PzMt/e3slm0bmzKenvNPreyuw1RnPM6LPju0Hzz0fn1kZ7rNG5tNHPXhVz1KrYgU0ppWZTz4eznzvVUwBmjnICpignYIpyAqYoJ2CKcgKmSo1Syl6FV0aZUcn58+dl/ou33pL50dGRzP/68KHMW838lXHh6lKt3BGPZcYd47EeV0QrZdEgZTLJf/5kon/uZrMVfXqQ68/v9XrZbHlpWT77l6++Cn7t0/HmBExRTsAU5QRMUU7AFOUETFFOwBTlBEyVmnPOco7Z6XRkvr62JvPnvv9cNnvx2jX57Le738p8MNAz1narLfPhMH88ZXSVXXzNXnREpH5e5cOhnnP2+/rYzYWFBZmvrKxks+jvWnTkZzTHDGe4jTPZrN3Wf96ff/GFzHN4cwKmKCdginICpignYIpyAqYoJ2CKcgKmSs05f/jSSzL/0fXr2azR0Lt/rWg/Lzh1syiKbBbtW0ZX1UWzxDK7ptGz0Wmj8dGY089Bo+MpLz77rMxbrfwea0opbd7fymZra6vy2Wi23B/0ZR6N7DsL+bn7x7dv64enxJsTMEU5AVOUEzBFOQFTlBMwRTkBU5QTMCXnnNFVdtdfeUXm3W43m43Heh43GpW7wk/N65ri3Nh//dp6ty/aa4zmgWoUWanogVs0B50E57NWg3xJnMG6tprft0xJzylTSumDD/8g84ODg2z28zfflM+qXdCUUhqN9Pcaza7r4mrGu5/dk89OizcnYIpyAqYoJ2CKcgKmKCdginICpuQopV7XG2VRrkSrTWWfV0cpdrv569xSilfGorWrk5OT4HmRBf9LvxF8582GHhPNzc3JvOjnV+1+/e678tnd3V2Zl7G39w+Zr5zTo5Si0H8mi2cWZX7v3mzGJQpvTsAU5QRMUU7AFOUETFFOwBTlBExRTsCUHJqpFZ6UUtrf1/niYn52FB7xGK2UBVe2qTlotBIWXTc3DtaP2m19rKeaD9dqwWw5WEdbXl6S+e9+/4HM//jnP8m8jGgtS33vA3FtYkrxNXyj4M9UHaWaUkqf3r0r81ngzQmYopyAKcoJmKKcgCnKCZiinIApygmYKnUF4Hu3fiPzdis/71td1Ve6Xb58WebR/p6ae53p5K9zSymlVjCnjFZRJ8FMrSf2PQ8P9ex4Z+eRzO98+onMy6gGw+lxiR3byNbWlszngjlntIO7eX/zf/2R/u2z9fcy7e4yb07AFOUETFFOwBTlBExRTsAU5QRMUU7AVEXNYN547eVyh8vCTpmdyrJzTpzuo9tfnvrF8uYETFFOwBTlBExRTsAU5QRMUU7AFOUETJXa58R3T5mdSuaY/1+8OQFTlBMwRTkBU5QTMEU5AVOUEzBFOQFTlBMwRTkBU5QTMEU5AVOUEzBFOQFTlBMwRTkBU5QTMEU5AVOUEzBFOQFTlBMwRTkBU5QTMCWvAATw5PDmBExRTsAU5QRMUU7AFOUETFFOwNQ/AYKhOUXA5t/iAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper1.imshow(image[11,:]);\n",
    "#see different items "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exo1: Building the network\n",
    "\n",
    "Here you should define your network. As with MNIST, \n",
    "each image is 28x28 which is a total of 784 pixels, \n",
    "and there are 10 classes. \n",
    "You should include at least one hidden layer. \n",
    "\n",
    "Suggest:  use ReLU activations for the layers and to return the logits or log-softmax from the forward pass. \n",
    "\n",
    "It's up to you how many layers you add and the size of those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your network architecture here\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "# Get the class probabilities\n",
    "ps = torch.exp(model(images))\n",
    "# Make sure the shape is appropriate, we should get 10 class probabilities for 64 examples\n",
    "print(ps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "Now you should create your network and train it. First you'll want to define [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) ( something like `nn.CrossEntropyLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (typically `optim.SGD` or `optim.Adam`).\n",
    "\n",
    "Then write the training code. Remember the training pass is a fairly straightforward process:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get the training loss below 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the network, define the criterion and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20..  Training Loss: 0.607..  Test Loss: 0.516..  Test Accuracy: 0.810\n",
      "Epoch: 2/20..  Training Loss: 0.491..  Test Loss: 0.461..  Test Accuracy: 0.840\n",
      "Epoch: 3/20..  Training Loss: 0.450..  Test Loss: 0.415..  Test Accuracy: 0.850\n",
      "Epoch: 4/20..  Training Loss: 0.429..  Test Loss: 0.415..  Test Accuracy: 0.851\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e88b238011f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \"\"\"\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'1'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m255\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     \u001b[1;31m# put it from HWC to CHW format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: Train the network here\n",
    "epochs = 20\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images, labels in testloader:\n",
    "                log_ps = model(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                \n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "        test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of overfitting or underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_correct = 0\n",
    "total_images = 0\n",
    "confusion_matrix = torch.zeros(10,10,dtype=torch.int64)\n",
    "confusion_matrix\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_images += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        print('Correct predictions',total_correct)\n",
    "\n",
    "        for i, l in enumerate(labels):\n",
    "            confusion_matrix[l.item()-1, predicted[i].item()-1] += 1\n",
    "            \n",
    "\n",
    "model_accuracy = total_correct / total_images * 100\n",
    "print('Model accuracy on {0} test images: {1:.2f}%'.format(total_images, model_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Test out your network!\n",
    "model.eval()\n",
    "\n",
    "# Get our data\n",
    "dataiter = iter(testloader)\n",
    "\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.view(1, 784)\n",
    "\n",
    "\n",
    "# TODO: Calculate the class probabilities (softmax) for img\n",
    "with torch.no_grad():\n",
    "    output = model.forward(img)\n",
    "    \n",
    "ps =torch.exp(output)\n",
    "\n",
    "\n",
    "# Plot the image and probabilities\n",
    "helper1.view_classify(img.resize_(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
